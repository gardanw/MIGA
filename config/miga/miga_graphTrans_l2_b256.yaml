_Base_Config: 'config/miga/iniv.yaml'
gpus: '1'
batch_size: 256
runner: NoiseGraphPretrainRunner
atom_to_what: for_diffusion
make_one_hot: True
num_workers: 32
epochs: 2000
run:
  task: miga_transformer_b128_l3_atom60_d128_l2_h4
  resume: #'/rhome/lianyu.zhou/cache/MIGA/miga_transformer_b128_l3_atom60/2023-2-4-15-26-18/weight/model_700.pth'

network:
  emb_dim: 128
  imgEncoder:
    target_num: 128
  molEncoder:
    name: GraphTransformer
    hidden_mlp_dims: { 'X': 128, 'E': 128}
    input_dims: {'X': 60, 'E': 5}
    # The dimensions should satisfy dx % n_head == 0
    hidden_dims: { 'dx': 128, 'de': 64, 'n_head': 4, 'dim_ffX': 256, 'dim_ffE': 128}

    n_layers: 2
